import os
from task.celery_app import celery_app
from task.splitter_loader import LOADER_MAP, SPLITTER_MAP
from task.gen_ques import generate_questions_task
from task.gen_vector_chain import encode_text_task
from celery import chain, group
from utils.logger_manager import get_logger
import asyncio
from db_service.pg_pool import pg_conn
from utils.vector_utils import vector_to_pgstring  # å‡è®¾ä½ å°è£…äº†è½¬æ¢å‡½æ•°
from celery import chord
import psycopg2
from psycopg2.extras import execute_batch
from config.settings import load_config

@celery_app.task(
    name="parse.file",
    bind=True,
    max_retries=3,
    autoretry_for=(Exception,)
)
def parse_file_and_enqueue_chunks(self, file_path: str, ext: str, file_id: str):
    logger = get_logger("parse_file_and_enqueue_chunks")
    try:
        if ext not in LOADER_MAP or ext not in SPLITTER_MAP:
            msg = f"æš‚ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹: .{ext}"
            logger.warning(f"[{file_id}] {msg}")
            return {"status": "skipped", "reason": msg}

        # âœ… åŠ è½½å™¨ & åˆ†æ®µå™¨
        loader = LOADER_MAP[ext]
        splitter = SPLITTER_MAP[ext]

        docs = loader(file_path)
        if not docs or all(not d.page_content.strip() for d in docs):
            msg = "æ–‡æ¡£å†…å®¹ä¸ºç©ºï¼Œæ— æ³•å¤„ç†"
            logger.warning(f"[{file_id}] {msg}")
            return {"status": "skipped", "reason": msg}

        chunks = splitter(docs)
        if not chunks:
            msg = "æ–‡æ¡£åˆ†æ®µä¸ºç©ºï¼Œè·³è¿‡å¤„ç†"
            logger.warning(f"[{file_id}] {msg}")
            return {"status": "skipped", "reason": msg}


        for chunk in chunks:
            # âœ… æ¯ä¸ª chunk å•ç‹¬è°ƒåº¦æ‰§è¡Œï¼Œæ— åµŒå¥—
            build_chunk_chain(chunk.page_content, file_id).apply_async()


        return {"status": "dispatched", "chunks": len(chunks)}


    except Exception as e:
        logger.exception(f"[{file_id}] æ–‡æ¡£å¤„ç†å¼‚å¸¸: {e}")
        raise self.retry(exc=e)

    finally:
        try:
            os.remove(file_path)
            logger.info(f"[{file_id}] å·²åˆ é™¤ä¸´æ—¶æ–‡ä»¶: {file_path}")
        except Exception as e:
            logger.warning(f"[{file_id}] ä¸´æ—¶æ–‡ä»¶åˆ é™¤å¤±è´¥: {e}")

def build_chunk_chain(text: str, file_id: str):
    return chain(
        generate_questions_task.s(text),
        encode_questions_and_store.s(file_id=file_id)
    )


# @celery_app.task(name="encode_and_insert", bind=True, autoretry_for=(Exception,), max_retries=3)
# def encode_questions_and_store(self, questions: list, file_id: str):
#     logger = get_logger("encode_and_insert")
#     try:
#         if not questions:
#             logger.warning(f"[{file_id}] æ— é—®é¢˜ç”Ÿæˆï¼Œè·³è¿‡ç¼–ç ä¸å…¥åº“")
#             return "skipped"
#
#         # ä¸ºæ¯ä¸ªé—®é¢˜åˆ›å»º encode å‘é‡åŒ–ä»»åŠ¡
#         encode_jobs = [encode_text_task.s(q) for q in questions]
#
#
#         logger.info(f"[{file_id}] âœ‰ï¸ å·²ç”Ÿæˆå‘é‡ä»»åŠ¡ chordï¼Œé—®é¢˜æ•°: {len(questions)}")
#
#         return chord(encode_jobs)(
#             insert_ques_batch_task.s(questions=questions, uu_id=file_id)
#         )
#     except Exception as e:
#         logger.exception(f"[{file_id}] é—®é¢˜å‘é‡åŒ–ä»»åŠ¡å¤±è´¥: {e}")
#         raise self.retry(exc=e)




@celery_app.task(name="encode_and_insert_each", bind=True, autoretry_for=(Exception,), max_retries=3)
def encode_questions_and_store(self, questions: list, file_id: str):
    logger = get_logger("encode_and_insert_each")
    try:
        if not questions:
            logger.warning(f"[{file_id}] æ— é—®é¢˜ç”Ÿæˆï¼Œè·³è¿‡ç¼–ç ä¸å…¥åº“")
            return "skipped"

        for question in questions:
            chain(
                encode_text_task.s(question),
                wrap_vector_as_list.s(),
                insert_ques_batch_task.s(questions=[question], uu_id=file_id)
            ).apply_async()

        logger.info(f"[{file_id}] âœ… å…±è°ƒåº¦ {len(questions)} æ¡ encodeâ†’insert å­ä»»åŠ¡é“¾")
        return f"dispatched {len(questions)} tasks"

    except Exception as e:
        logger.exception(f"[{file_id}] encode+insert ä»»åŠ¡é“¾è°ƒåº¦å¤±è´¥: {e}")
        raise self.retry(exc=e)




@celery_app.task(name="insert.ques.batch", bind=True, autoretry_for=(Exception,), max_retries=3)
def insert_ques_batch_task(self, vectors, *, questions, uu_id):
    # questions = kwargs.get("questions")
    # uu_id = kwargs.get("uu_id")
    logger = get_logger("insert_ques_batch")
    # logger.info(f"[{uu_id}] âœ… insert task è¢«è°ƒç”¨")
    # logger.info(f"[{uu_id}] ğŸ‘€ vectors: {vectors}")
    # logger.info(f"[{uu_id}] ğŸ‘€ questions: {questions}")

    try:
        # âœ… ä¿®å¤æ ¸å¿ƒ bugï¼šåŒ…è£…å•ä¸ªå‘é‡ä¸ºäºŒç»´ list
        if not isinstance(vectors[0], (list, tuple)):
            vectors = [vectors]

        config = load_config()
        db_cfg = config.wmx_database

        conn = psycopg2.connect(
            dbname=db_cfg.DB_NAME,
            user=db_cfg.DB_USER,
            password=db_cfg.DB_PASSWORD,
            host=db_cfg.DB_HOST,
            port=db_cfg.DB_PORT
        )
        cursor = conn.cursor()

        sql = """
            INSERT INTO wmx_ques (ori_sent_id, ori_ques_sent, ques_vector)
            VALUES (%s, %s, %s)
        """
        data = [
            (uu_id, q, vector_to_pgstring(v))
            for q, v in zip(questions, vectors)
        ]

        execute_batch(cursor, sql, data)
        conn.commit()
        logger.info(f"[{uu_id}] âœ… æˆåŠŸå†™å…¥ {len(data)} æ¡é—®é¢˜å‘é‡")

    except Exception as e:
        logger.exception(f"[{uu_id}] âŒ å‘é‡å…¥åº“å¤±è´¥: {e}")
        raise self.retry(exc=e)

    finally:
        if 'cursor' in locals():
            cursor.close()
        if 'conn' in locals():
            conn.close()

@celery_app.task(name="wrap.vector")
def wrap_vector_as_list(vec):
    return [vec]

