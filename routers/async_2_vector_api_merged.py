import asyncio
import os
from typing import List
from fastapi import APIRouter, HTTPException, Depends, Query
from sqlalchemy.ext.asyncio import AsyncSession
# from db_service.session import get_async_db
# from db_service.db_search_service import async_query_similar_sentences, async_hybrid_search
from elasticsearch import AsyncElasticsearch
from task.es_fun.search_engine import search_bm25, search_vector, merge_results
from utils.logger_manager import get_logger
from redis import Redis
import json
# from task.vector_tasks import encode_text_task
from task.gen_vector_chain import encode_text_task
from config.settings import settings
import time
from db_service.db_interact_service import (
    insert_vectors_to_db,
    insert_ques_batch,
    update_by_id,
    update_field_by_id
)
from routers.schema import (
    VectorItem,
    ResponseModel,
    InsertVectorItem,
    InsertQuesBatchItem,
    UpdateByIdItem,
    FileBatchRequest,
    WriteQuesBatch,
    SearchRequest,
    SearchResult
)
import aiohttp
import tempfile
from task.file_parse_pipeline_new import parse_file_and_enqueue_chunks
from utils.task_utils import submit_vector_task_with_option
from task.es_fun.es_delete import delete_doc, delete_by_term, delete_by_terms
from celery import chain
from db_service.pg_pool import pg_conn
from celery.result import AsyncResult
from task.celery_app import celery_app
router = APIRouter()
# logger = setup_logger("async_vector_api")
logger = get_logger("router_async_vector_api")
# logger.info("ä»»åŠ¡å¼€å§‹")


# Redis client
redis_client = Redis.from_url(settings.vector_service.redis_backend)


es = AsyncElasticsearch(
    hosts=[settings.elasticsearch.host],
    http_auth=(settings.elasticsearch.username, settings.elasticsearch.password)
)

# # âœ… æäº¤å¼‚æ­¥å‘é‡è®¡ç®—ä»»åŠ¡
# @router.post("/search/gen_vector", response_model=ResponseModel)
# def submit_vector_task(item: VectorItem):
#     try:
#         task = encode_text_task.delay(item.text)
#         task_id = task.id
#         redis_client.set(f"text:{task_id}", item.text, ex=3600)
#         return {"msg": "ä»»åŠ¡åˆ›å»ºæˆåŠŸ", "data": {"task_id": task_id}}
#     except Exception as e:
#         logger.exception("æäº¤å‘é‡è®¡ç®—ä»»åŠ¡å¤±è´¥")
#         return {"msg": "å¤„ç†å¤±è´¥", "data": {"error": str(e)}}





es_cfg = settings.elasticsearch
FILE_INDEX = es_cfg.indexes.file_index
CHUNK_INDEX = es_cfg.indexes.chunk_index
QUES_INDEX = es_cfg.indexes.ques_index

@router.post("/search/gen_vector", response_model=ResponseModel)
def submit_vector_task(item: VectorItem):
    try:
        task = submit_vector_task_with_option(item.text, write_to_redis=True)
        redis_client.set(f"text:{task.id}", item.text, ex=3600)
        return {"msg": "ä»»åŠ¡åˆ›å»ºæˆåŠŸ", "data": {"task_id": task.id}}
    except Exception as e:
        logger.exception("æäº¤å‘é‡è®¡ç®—ä»»åŠ¡å¤±è´¥")
        return {"msg": "å¤„ç†å¤±è´¥", "data": {"error": str(e)}}






@router.post("/db/insert_vector", response_model=ResponseModel)
async def insert_vector(item: InsertVectorItem):
    try:
        await insert_vectors_to_db(
            zhisk_file_id=item.zhisk_file_id,
            content=item.content,
            vector=item.vector,
            jsons=item.jsons,
            code=item.code,
            category=item.category,
            uu_id=item.uu_id
        )
        return {"msg": "æ’å…¥æˆåŠŸ", "data": None}
    except Exception as e:
        logger.exception("æ’å…¥å‘é‡è®°å½•å¤±è´¥")
        return {"msg": "å¤±è´¥", "data": {"error": str(e)}}


@router.post("/db/insert_ques_batch", response_model=ResponseModel)
async def insert_batch(item: InsertQuesBatchItem):
    try:
        await insert_ques_batch(item.uu_id, item.sentences, item.vectors)
        return {"msg": "æ‰¹é‡æ’å…¥æˆåŠŸ", "data": None}
    except Exception as e:
        logger.exception("æ‰¹é‡æ’å…¥é—®é¢˜å¤±è´¥")
        return {"msg": "å¤±è´¥", "data": {"error": str(e)}}


@router.post("/db/update_by_id", response_model=ResponseModel)
async def update_by_id_route(item: UpdateByIdItem):
    try:
        await update_by_id(item.update_data, item.record_id, item.updata_ques, item.up_ques_vect)
        return {"msg": "æ›´æ–°æˆåŠŸ", "data": None}
    except Exception as e:
        logger.exception("æ›´æ–°è®°å½•å¤±è´¥")
        return {"msg": "å¤±è´¥", "data": {"error": str(e)}}


@router.post("/db/update_field", response_model=ResponseModel)
async def update_field_route(
    table_name: str = Query(...),
    field_name: str = Query(...),
    new_value: str = Query(...),
    record_id: int = Query(...)
):
    try:
        await update_field_by_id(table_name, field_name, new_value, record_id)
        return {"msg": "å­—æ®µæ›´æ–°æˆåŠŸ", "data": None}
    except Exception as e:
        logger.exception("å­—æ®µæ›´æ–°å¤±è´¥")
        return {"msg": "å¤±è´¥", "data": {"error": str(e)}}




# @router.post("/upload")
# async def upload_and_dispatch_files(data: FileBatchRequest):
#     task_ids = []
#
#     for file in data.files:
#         ext = file.filename.split(".")[-1].lower()
#
#         async with aiohttp.ClientSession() as session:
#             async with session.get(file.url) as resp:
#                 if resp.status != 200:
#                     raise HTTPException(status_code=400, detail=f"ä¸‹è½½å¤±è´¥: {file.filename}")
#                 with tempfile.NamedTemporaryFile(delete=False, suffix=f".{ext}") as tmp:
#                     tmp.write(await resp.read())
#                     tmp.flush()
#                     os.fsync(tmp.fileno())
#                     tmp_path = tmp.name
#
#         # æ„å»ºä»»åŠ¡é“¾ï¼ˆè¿™é‡Œåªä½¿ç”¨äº†ä¸€ä¸ªä»»åŠ¡,å¯ç»§ç»­æ·»åŠ ï¼‰
#         task_chain = chain(
#             parse_file_and_enqueue_chunks.s(tmp_path, ext, file.file_id)
#             # å¦‚æœä½ æœ‰å…¶å®ƒæ”¶å°¾ä»»åŠ¡ï¼Œæ¯”å¦‚çŠ¶æ€æ›´æ–°ã€å…¥åº“è®°å½•ç­‰ï¼Œå¯ä»¥ç»§ç»­åŠ åˆ°è¿™é‡Œ
#         )
#
#         result = task_chain.apply_async()
#         task_ids.append({
#             "file_id": file.file_id,
#             "task_id": result.id,   # chain çš„ root id
#             "filename": file.filename
#         })
#
#     return {"msg": "ä¸Šä¼ ä»»åŠ¡å·²æäº¤", "tasks": task_ids}


@router.post("/upload")
async def upload_and_dispatch_files(data: FileBatchRequest):
    task_ids = []

    async with aiohttp.ClientSession() as session:
        for file in data.files:
            ext = file.filename.split(".")[-1].lower()

            async with session.get(file.url) as resp:
                if resp.status != 200:
                    raise HTTPException(status_code=400, detail=f"ä¸‹è½½å¤±è´¥: {file.filename}")

                with tempfile.NamedTemporaryFile(delete=False, suffix=f".{ext}") as tmp:
                    tmp.write(await resp.read())
                    tmp.flush()
                    os.fsync(tmp.fileno())
                    tmp_path = tmp.name

            # âœ… ä¸å†æ„é€  store_targetï¼Œä»»åŠ¡å†…éƒ¨ä½¿ç”¨ settings.task_defaults æ§åˆ¶
            task_chain = parse_file_and_enqueue_chunks.s(
                tmp_path,
                ext,
                create_by=file.user_id  # âœ… ä¿ç•™ user_id å­—æ®µç”¨äºå®¡è®¡
            )

            result = task_chain.apply_async()
            task_ids.append({
                "file_id": file.file_id,
                "task_id": result.id,
                "filename": file.filename
            })

    return {"msg": "ä¸Šä¼ ä»»åŠ¡å·²æäº¤", "tasks": task_ids}

# @router.get("/task_status/{task_id}")
# def get_task_status(task_id: str):
#     result = AsyncResult(task_id, app=celery_app)
#     return {
#         "task_id": task_id,
#         "status": result.status,  # PENDING, STARTED, SUCCESS, FAILURE, RETRY
#         "result": result.result if result.successful() else None,
#         "error": str(result.result) if result.failed() else None
#     }
# @router.post("/db/write-ques-batch")
# async def write_ques_batch(payload: WriteQuesBatch):
#     if len(payload.sentences) != len(payload.vectors):
#         return {"error": "æ•°é‡ä¸ä¸€è‡´"}
#
#     data = [
#         (payload.uu_id, sent, json.dumps(vec))
#         for sent, vec in zip(payload.sentences, payload.vectors)
#     ]
#     sql = "INSERT INTO wmx_ques (ori_sent_id, ori_ques_sent, ques_vector) VALUES ($1, $2, $3)"
#     async with pg_conn() as conn:
#         async with conn.transaction():
#             await conn.executemany(sql, data)
#
#     return {"status": "ok", "inserted": len(data)}


@router.post("/es_hybrid_search", response_model=List[SearchResult])
async def hybrid_search_api(request: SearchRequest):
    # âœ… æ ¡éªŒï¼šè‡³å°‘è¦å¯ç”¨ä¸€ç§æ£€ç´¢æ–¹å¼
    if not (request.use_bm25 or request.use_vector):
        raise HTTPException(status_code=400, detail="å¿…é¡»å¯ç”¨è‡³å°‘ä¸€ç§æ£€ç´¢æ–¹å¼ï¼ˆuse_bm25 æˆ– use_vectorï¼‰")

    tasks = []
    if request.use_bm25:
        tasks.append(search_bm25(request.query))
    else:
        tasks.append(asyncio.sleep(0, result=[]))

    if request.use_vector:
        tasks.append(search_vector(request.query))
    else:
        tasks.append(asyncio.sleep(0, result=[]))

    bm25_results, vector_results = await asyncio.gather(*tasks)
    print("ğŸ¯ vector åŸå§‹å¾—åˆ†:")
    for r in vector_results:
        print(f"{r['id']} -> {r['score']}")
    merged = merge_results(bm25_results, vector_results, alpha=request.alpha)
    # return merged
    return [SearchResult(**item) for item in merged]  # âœ… ä¿è¯ç»“æ„



# ========== åˆ é™¤é—®é¢˜ ==========
@router.delete("/delete/question")
async def delete_questions(question_ids: List[str]):
    for qid in question_ids:
        await delete_doc(QUES_INDEX, qid)
    return {"deleted_questions": question_ids}


# ========== åˆ é™¤ chunkï¼ˆåŠå…¶é—®é¢˜ï¼‰ ==========
@router.delete("/delete/chunk")
async def delete_chunks(chunk_uuids: List[str]):
    for uuid in chunk_uuids:
        # åˆ é™¤ä¸‹å±é—®é¢˜
        await delete_by_term(QUES_INDEX, "ori_sent_id", uuid)
    await delete_by_terms(CHUNK_INDEX, "uu_id", chunk_uuids)
    return {"deleted_chunks": chunk_uuids}


# ========== åˆ é™¤æ–‡ä»¶ï¼ˆåŠå…¶ chunk å’Œé—®é¢˜ï¼‰ ==========
@router.delete("/delete/file")
async def delete_files(zhisk_file_ids: List[str]):
    for file_id in zhisk_file_ids:
        # æŸ¥å‡ºè¯¥æ–‡ä»¶æ‰€æœ‰ chunk çš„ uuidï¼ˆç”¨äºåˆ é™¤å¯¹åº”é—®é¢˜ï¼‰
        resp = await es.search(index=CHUNK_INDEX, query={
            "term": {"zhisk_file_id": file_id}
        }, size=1000, _source=["uu_id"])
        chunk_uuids = [hit["_source"]["uu_id"] for hit in resp["hits"]["hits"]]

        # åˆ é™¤é—®é¢˜
        if chunk_uuids:
            await delete_by_terms(QUES_INDEX, "ori_sent_id", chunk_uuids)

        # åˆ é™¤ chunk
        await delete_by_term(CHUNK_INDEX, "zhisk_file_id", file_id)

        # åˆ é™¤æ–‡ä»¶å…ƒä¿¡æ¯
        await delete_doc(FILE_INDEX, file_id)

    return {"deleted_files": zhisk_file_ids}


