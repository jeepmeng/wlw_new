import asyncio
import os
from typing import List
from fastapi import APIRouter, HTTPException, Depends, Query
from sqlalchemy.ext.asyncio import AsyncSession
# from db_service.session import get_async_db
# from db_service.db_search_service import async_query_similar_sentences, async_hybrid_search
from task.es_fun.search_engine import search_bm25, search_vector, merge_results
from utils.logger_manager import get_logger
from redis import Redis
import json
# from task.vector_tasks import encode_text_task
from task.gen_vector_chain import encode_text_task
from config.settings import settings
import time
from db_service.db_interact_service import (
    insert_vectors_to_db,
    insert_ques_batch,
    update_by_id,
    update_field_by_id
)
from routers.schema import (
    VectorItem,
    ResponseModel,
    InsertVectorItem,
    InsertQuesBatchItem,
    UpdateByIdItem,
    FileBatchRequest,
    WriteQuesBatch,
    SearchRequest,
    SearchResult
)
import aiohttp
import tempfile
from task.file_parse_pipeline_new import parse_file_and_enqueue_chunks
from utils.task_utils import submit_vector_task_with_option
from celery import chain
from db_service.pg_pool import pg_conn
from celery.result import AsyncResult
from task.celery_app import celery_app
router = APIRouter()
# logger = setup_logger("async_vector_api")
logger = get_logger("router_async_vector_api")
# logger.info("ä»»åŠ¡å¼€å§‹")


# Redis client
redis_client = Redis.from_url(settings.vector_service.redis_backend)




# # âœ… æäº¤å¼‚æ­¥å‘é‡è®¡ç®—ä»»åŠ¡
# @router.post("/search/gen_vector", response_model=ResponseModel)
# def submit_vector_task(item: VectorItem):
#     try:
#         task = encode_text_task.delay(item.text)
#         task_id = task.id
#         redis_client.set(f"text:{task_id}", item.text, ex=3600)
#         return {"msg": "ä»»åŠ¡åˆ›å»ºæˆåŠŸ", "data": {"task_id": task_id}}
#     except Exception as e:
#         logger.exception("æäº¤å‘é‡è®¡ç®—ä»»åŠ¡å¤±è´¥")
#         return {"msg": "å¤„ç†å¤±è´¥", "data": {"error": str(e)}}




@router.post("/search/gen_vector", response_model=ResponseModel)
def submit_vector_task(item: VectorItem):
    try:
        task = submit_vector_task_with_option(item.text, write_to_redis=True)
        redis_client.set(f"text:{task.id}", item.text, ex=3600)
        return {"msg": "ä»»åŠ¡åˆ›å»ºæˆåŠŸ", "data": {"task_id": task.id}}
    except Exception as e:
        logger.exception("æäº¤å‘é‡è®¡ç®—ä»»åŠ¡å¤±è´¥")
        return {"msg": "å¤„ç†å¤±è´¥", "data": {"error": str(e)}}


# âœ… è½®è¯¢è·å–ä»»åŠ¡è®¡ç®—ç»“æœå¹¶æ‰§è¡Œå‘é‡æŸ¥è¯¢
# @router.get("/search/vector_search/{task_id}", response_model=ResponseModel)
# async def get_vector_result(task_id: str, db: AsyncSession = Depends(get_async_db)):
#     try:
#         redis_key = f"vec_result:{task_id}"
#         vec_json = redis_client.get(redis_key)
#         if vec_json:
#             text_vec = json.loads(vec_json)
#             results = await async_query_similar_sentences(text_vec, db)
#             return {"msg": "top-10 vector search", "data": {"results": results}}
#         else:
#             return {"msg": "ä»»åŠ¡æœªå®Œæˆ", "data": {"status": "PENDING"}}
#     except Exception as e:
#         logger.exception(f"è·å–å‘é‡ç»“æœå¤±è´¥ï¼Œtask_id={task_id}")
#         return {"msg": "å¤„ç†å¤±è´¥", "data": {"error": str(e)}}

# @router.get("/search/vector_search/{task_id}", response_model=ResponseModel)
# async def get_vector_result(task_id: str):
#     try:
#         redis_key = f"vec_result:{task_id}"
#         vec_json = redis_client.get(redis_key)
#         if vec_json:
#             text_vec = json.loads(vec_json)
#             async with pg_conn() as conn:
#                 async with conn.transaction():
#                     results = await async_query_similar_sentences(text_vec, conn)
#             return {"msg": "top-10 vector search", "data": {"results": results}}
#         else:
#             return {"msg": "ä»»åŠ¡æœªå®Œæˆ", "data": {"status": "PENDING"}}
#     except Exception as e:
#         logger.exception(f"è·å–å‘é‡ç»“æœå¤±è´¥ï¼Œtask_id={task_id}")
#         return {"msg": "å¤„ç†å¤±è´¥", "data": {"error": str(e)}}


# # âœ… æäº¤æ··åˆæœç´¢ä»»åŠ¡ï¼ˆä¿å­˜ query_textï¼‰
# @router.post("/vector/mix_task", response_model=ResponseModel)
# def submit_mix_task(item: VectorItem):
#     task = encode_text_task.delay(item.text)
#     redis_client.set(f"text:{task.id}", item.text, ex=3600)
#     return {"msg": "ä»»åŠ¡åˆ›å»ºæˆåŠŸ", "data": {"task_id": task.id}}


# @router.post("/vector/hybrid_search", response_model=ResponseModel)
# async def submit_mix_task_blocking(
#         item: VectorItem,
#         db: AsyncSession = Depends(get_async_db),
#         top_k: int = Query(15, ge=1),
#         lambda_: float = Query(0.8, ge=0.0, le=1.0),
#         timeout: int = Query(30, ge=1, le=60)
# ):
#     try:
#         task = encode_text_task.delay(item.text)
#         task_id = task.id
#         redis_client.set(f"text:{task_id}", item.text, ex=3600)
#
#         start = time.time()
#         while time.time() - start < timeout:
#             vec_json = redis_client.get(f"vec_result:{task_id}")
#             if vec_json:
#                 vector = json.loads(vec_json)
#                 results = await async_hybrid_search(item.text, vector, db, top_k=top_k, lambda_=lambda_)
#                 return {"msg": "hybrid search result", "data": {"results": results}}
#             await asyncio.sleep(1)
#
#         return {"msg": "ç­‰å¾…è¶…æ—¶ï¼Œä»»åŠ¡æœªå®Œæˆ", "data": {"task_id": task_id, "status": "TIMEOUT"}}
#     except Exception as e:
#         logger.exception("é˜»å¡å¼æ··åˆæœç´¢å¤±è´¥")
#         return {"msg": "å¤„ç†å¤±è´¥", "data": {"error": str(e)}}


# @router.post("/vector/hybrid_search", response_model=ResponseModel)
# async def submit_mix_task_blocking(item: VectorItem, top_k: int = Query(15, ge=1), lambda_: float = Query(0.8, ge=0.0, le=1.0), timeout: int = Query(30, ge=1, le=60)):
#     try:
#         task = encode_text_task.delay(item.text)
#         task_id = task.id
#         redis_client.set(f"text:{task_id}", item.text, ex=3600)
#
#         start = time.time()
#         while time.time() - start < timeout:
#             vec_json = redis_client.get(f"vec_result:{task_id}")
#             if vec_json:
#                 vector = json.loads(vec_json)
#                 async with pg_conn() as conn:
#                     async with conn.transaction():
#                         results = await async_hybrid_search(item.text, vector, conn, top_k=top_k, lambda_=lambda_)
#                 return {"msg": "hybrid search result", "data": {"results": results}}
#             await asyncio.sleep(1)
#
#         return {"msg": "ç­‰å¾…è¶…æ—¶ï¼Œä»»åŠ¡æœªå®Œæˆ", "data": {"task_id": task_id, "status": "TIMEOUT"}}
#     except Exception as e:
#         logger.exception("é˜»å¡å¼æ··åˆæœç´¢å¤±è´¥")
#         return {"msg": "å¤„ç†å¤±è´¥", "data": {"error": str(e)}}



# âœ… æŸ¥è¯¢æ··åˆæœç´¢ç»“æœï¼ˆæ”¯æŒå¯é€‰å‚æ•°ï¼‰
# @router.get("/vector/hybrid_search/{task_id}", response_model=ResponseModel)
# async def get_hybrid_search_result(
#         task_id: str,
#         db: AsyncSession = Depends(get_async_db),
#         top_k: int = Query(15, ge=1),
#         lambda_: float = Query(0.8, ge=0.0, le=1.0)
# ):
#     try:
#         vec_json = redis_client.get(f"vec_result:{task_id}")
#         query_text = redis_client.get(f"text:{task_id}")
#
#         if not vec_json or not query_text:
#             return {"msg": "ä»»åŠ¡æœªå®Œæˆ", "data": {"status": "PENDING"}}
#
#         vector = json.loads(vec_json)
#         text = query_text.decode()
#         results = await async_hybrid_search(text, vector, db, top_k=top_k, lambda_=lambda_)
#         return {"msg": "hybrid search result", "data": {"results": results}}
#     except Exception as e:
#         logger.exception(f"è·å–æ··åˆæœç´¢ç»“æœå¤±è´¥ï¼Œtask_id={task_id}")
#         return {"msg": "å¤„ç†å¤±è´¥", "data": {"error": str(e)}}



# @router.get("/vector/hybrid_search/{task_id}", response_model=ResponseModel)
# async def get_hybrid_search_result(task_id: str, top_k: int = Query(15, ge=1), lambda_: float = Query(0.8, ge=0.0, le=1.0)):
#     try:
#         vec_json = redis_client.get(f"vec_result:{task_id}")
#         query_text = redis_client.get(f"text:{task_id}")
#
#         if not vec_json or not query_text:
#             return {"msg": "ä»»åŠ¡æœªå®Œæˆ", "data": {"status": "PENDING"}}
#
#         vector = json.loads(vec_json)
#         text = query_text.decode()
#
#         async with pg_conn() as conn:
#             async with conn.transaction():
#                 results = await async_hybrid_search(text, vector, conn, top_k=top_k, lambda_=lambda_)
#         return {"msg": "hybrid search result", "data": {"results": results}}
#     except Exception as e:
#         logger.exception(f"è·å–æ··åˆæœç´¢ç»“æœå¤±è´¥ï¼Œtask_id={task_id}")
#         return {"msg": "å¤„ç†å¤±è´¥", "data": {"error": str(e)}}



@router.post("/db/insert_vector", response_model=ResponseModel)
async def insert_vector(item: InsertVectorItem):
    try:
        await insert_vectors_to_db(
            zhisk_file_id=item.zhisk_file_id,
            content=item.content,
            vector=item.vector,
            jsons=item.jsons,
            code=item.code,
            category=item.category,
            uu_id=item.uu_id
        )
        return {"msg": "æ’å…¥æˆåŠŸ", "data": None}
    except Exception as e:
        logger.exception("æ’å…¥å‘é‡è®°å½•å¤±è´¥")
        return {"msg": "å¤±è´¥", "data": {"error": str(e)}}


@router.post("/db/insert_ques_batch", response_model=ResponseModel)
async def insert_batch(item: InsertQuesBatchItem):
    try:
        await insert_ques_batch(item.uu_id, item.sentences, item.vectors)
        return {"msg": "æ‰¹é‡æ’å…¥æˆåŠŸ", "data": None}
    except Exception as e:
        logger.exception("æ‰¹é‡æ’å…¥é—®é¢˜å¤±è´¥")
        return {"msg": "å¤±è´¥", "data": {"error": str(e)}}


@router.post("/db/update_by_id", response_model=ResponseModel)
async def update_by_id_route(item: UpdateByIdItem):
    try:
        await update_by_id(item.update_data, item.record_id, item.updata_ques, item.up_ques_vect)
        return {"msg": "æ›´æ–°æˆåŠŸ", "data": None}
    except Exception as e:
        logger.exception("æ›´æ–°è®°å½•å¤±è´¥")
        return {"msg": "å¤±è´¥", "data": {"error": str(e)}}


@router.post("/db/update_field", response_model=ResponseModel)
async def update_field_route(
    table_name: str = Query(...),
    field_name: str = Query(...),
    new_value: str = Query(...),
    record_id: int = Query(...)
):
    try:
        await update_field_by_id(table_name, field_name, new_value, record_id)
        return {"msg": "å­—æ®µæ›´æ–°æˆåŠŸ", "data": None}
    except Exception as e:
        logger.exception("å­—æ®µæ›´æ–°å¤±è´¥")
        return {"msg": "å¤±è´¥", "data": {"error": str(e)}}




# @router.post("/upload")
# async def upload_and_dispatch_files(data: FileBatchRequest):
#     task_ids = []
#
#     for file in data.files:
#         ext = file.filename.split(".")[-1].lower()
#
#         async with aiohttp.ClientSession() as session:
#             async with session.get(file.url) as resp:
#                 if resp.status != 200:
#                     raise HTTPException(status_code=400, detail=f"ä¸‹è½½å¤±è´¥: {file.filename}")
#                 with tempfile.NamedTemporaryFile(delete=False, suffix=f".{ext}") as tmp:
#                     tmp.write(await resp.read())
#                     tmp.flush()
#                     os.fsync(tmp.fileno())
#                     tmp_path = tmp.name
#
#         # æ„å»ºä»»åŠ¡é“¾ï¼ˆè¿™é‡Œåªä½¿ç”¨äº†ä¸€ä¸ªä»»åŠ¡,å¯ç»§ç»­æ·»åŠ ï¼‰
#         task_chain = chain(
#             parse_file_and_enqueue_chunks.s(tmp_path, ext, file.file_id)
#             # å¦‚æœä½ æœ‰å…¶å®ƒæ”¶å°¾ä»»åŠ¡ï¼Œæ¯”å¦‚çŠ¶æ€æ›´æ–°ã€å…¥åº“è®°å½•ç­‰ï¼Œå¯ä»¥ç»§ç»­åŠ åˆ°è¿™é‡Œ
#         )
#
#         result = task_chain.apply_async()
#         task_ids.append({
#             "file_id": file.file_id,
#             "task_id": result.id,   # chain çš„ root id
#             "filename": file.filename
#         })
#
#     return {"msg": "ä¸Šä¼ ä»»åŠ¡å·²æäº¤", "tasks": task_ids}


@router.post("/upload")
async def upload_and_dispatch_files(data: FileBatchRequest):
    task_ids = []

    async with aiohttp.ClientSession() as session:
        for file in data.files:
            ext = file.filename.split(".")[-1].lower()

            async with session.get(file.url) as resp:
                if resp.status != 200:
                    raise HTTPException(status_code=400, detail=f"ä¸‹è½½å¤±è´¥: {file.filename}")

                with tempfile.NamedTemporaryFile(delete=False, suffix=f".{ext}") as tmp:
                    tmp.write(await resp.read())
                    tmp.flush()
                    os.fsync(tmp.fileno())
                    tmp_path = tmp.name

            # âœ… ä¸å†æ„é€  store_targetï¼Œä»»åŠ¡å†…éƒ¨ä½¿ç”¨ settings.task_defaults æ§åˆ¶
            task_chain = parse_file_and_enqueue_chunks.s(
                tmp_path,
                ext,
                create_by=file.user_id  # âœ… ä¿ç•™ user_id å­—æ®µç”¨äºå®¡è®¡
            )

            result = task_chain.apply_async()
            task_ids.append({
                "file_id": file.file_id,
                "task_id": result.id,
                "filename": file.filename
            })

    return {"msg": "ä¸Šä¼ ä»»åŠ¡å·²æäº¤", "tasks": task_ids}

@router.get("/task_status/{task_id}")
def get_task_status(task_id: str):
    result = AsyncResult(task_id, app=celery_app)
    return {
        "task_id": task_id,
        "status": result.status,  # PENDING, STARTED, SUCCESS, FAILURE, RETRY
        "result": result.result if result.successful() else None,
        "error": str(result.result) if result.failed() else None
    }
@router.post("/db/write-ques-batch")
async def write_ques_batch(payload: WriteQuesBatch):
    if len(payload.sentences) != len(payload.vectors):
        return {"error": "æ•°é‡ä¸ä¸€è‡´"}

    data = [
        (payload.uu_id, sent, json.dumps(vec))
        for sent, vec in zip(payload.sentences, payload.vectors)
    ]
    sql = "INSERT INTO wmx_ques (ori_sent_id, ori_ques_sent, ques_vector) VALUES ($1, $2, $3)"
    async with pg_conn() as conn:
        async with conn.transaction():
            await conn.executemany(sql, data)

    return {"status": "ok", "inserted": len(data)}


@router.post("/es_hybrid_search", response_model=List[SearchResult])
async def hybrid_search_api(request: SearchRequest):
    # âœ… æ ¡éªŒï¼šè‡³å°‘è¦å¯ç”¨ä¸€ç§æ£€ç´¢æ–¹å¼
    if not (request.use_bm25 or request.use_vector):
        raise HTTPException(status_code=400, detail="å¿…é¡»å¯ç”¨è‡³å°‘ä¸€ç§æ£€ç´¢æ–¹å¼ï¼ˆuse_bm25 æˆ– use_vectorï¼‰")

    tasks = []
    if request.use_bm25:
        tasks.append(search_bm25(request.query))
    else:
        tasks.append(asyncio.sleep(0, result=[]))

    if request.use_vector:
        tasks.append(search_vector(request.query))
    else:
        tasks.append(asyncio.sleep(0, result=[]))

    bm25_results, vector_results = await asyncio.gather(*tasks)
    print("ğŸ¯ vector åŸå§‹å¾—åˆ†:")
    for r in vector_results:
        print(f"{r['id']} -> {r['score']}")
    merged = merge_results(bm25_results, vector_results, alpha=request.alpha)
    # return merged
    return [SearchResult(**item) for item in merged]  # âœ… ä¿è¯ç»“æ„