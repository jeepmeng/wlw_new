from fastapi import FastAPI, Request, Depends
from fastapi.responses import StreamingResponse, JSONResponse
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from openai import AsyncOpenAI
import asyncio
import logging
import threading
import json
from sentence_transformers import SentenceTransformer
from history import his_talk
from pg_interact import read_yaml, connect_db
from talk_insert_pg import insert_talk_vectors_to_db
from dialog_service.vector_engine import submit_vector_task_sync, get_vector_result_by_task, format_results_into_prompt
from sqlalchemy.ext.asyncio import AsyncSession
from db_service.session import get_async_db

# åˆå§‹åŒ–
app = FastAPI()

# CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# æ—¥å¿—é…ç½®
logging.basicConfig(
    format="%(asctime)s - %(levelname)s - [Thread: %(threadName)s] - %(message)s",
    level=logging.INFO
)
logger = logging.getLogger(__name__)

# åŠ è½½é…ç½®
yaml_file = "config.yaml"
db_params = read_yaml(yaml_file)
chat_sessions = {}

# è¯·æ±‚ç»“æ„
class ChatRequest(BaseModel):
    session_id: str
    file_text: str
    file_userId: int
    file_talkId: int

class ControlRequest(BaseModel):
    session_id: str
    action: str  # "pause", "resume", "stop"

# å¤šè½®å¯¹è¯ä¸»æ¥å£
@app.post("/roundTalk")
async def multiple_rounds_chat(request_data: ChatRequest, db: AsyncSession = Depends(get_async_db)):
    client = AsyncOpenAI(api_key="sk-3cd962207189494f872bdec8394a1ffe", base_url="https://api.deepseek.com")
    file_text = request_data.file_text
    file_userId = request_data.file_userId
    file_talkId = request_data.file_talkId
    session_id = request_data.session_id

    logger.info(f"å¤„ç†è¯·æ±‚çš„çº¿ç¨‹ ID: {threading.get_ident()}, çº¿ç¨‹åç§°: {threading.current_thread().name}")

    try:
        res = his_talk(file_userId, file_talkId, connect_db, db_params)
        messages = list(res)

        # æäº¤å¼‚æ­¥å‘é‡è®¡ç®—ä»»åŠ¡å¹¶è½®è¯¢è·å–ç»“æœ
        task_id = submit_vector_task_sync(file_text)
        vector_results = None
        for _ in range(20):
            vector_results = await get_vector_result_by_task(task_id, db)
            if vector_results:
                break
            await asyncio.sleep(0.5)

        zhishi_prompt = format_results_into_prompt(vector_results)

        new_prompt = (
            f"{zhishi_prompt}ä»¥ä¸Šå†…å®¹ä¸ºæœ¬è½®é—®é¢˜çŸ¥è¯†åº“ç›¸å…³å†…å®¹ï¼Œé‡ç‚¹å¼ºè°ƒ**å¦‚æœå†…å®¹åŒ…å«ï¼š//æ ‡å‡†æ¨¡ç‰ˆ ,ç›´æ¥å°†å†…å®¹æ ¼å¼åŒ–è¾“å‡ºç»™ç”¨æˆ·(æ ¼å¼è¦ç¼–æ’ä¸€ä¸‹)ï¼Œä¸è¦åšä»»ä½•æ”¹åŠ¨ã€‚å¦‚æœå†…å®¹æ²¡æœ‰ï¼š//æ ‡å‡†æ¨¡ç‰ˆï¼Œå¯ä½œä¸ºå‚è€ƒï¼Œå¦‚æœä¸é—®é¢˜æ— å…³è¯·å¿½ç•¥ï¼Œå¹¶æ­£å¸¸ä½œç­”ã€‚ä¸Šè¿°æç¤ºè¯ä¸è¦åœ¨æ€è€ƒé˜¶æ®µå¯¹å¤–å±•ç¤ºã€‚\n\næœ¬è½®çš„ç”¨æˆ·æé—®æ˜¯ï¼š{file_text}"
        )
        messages.append({"role": "user", "content": new_prompt})

        async def generate_response(file_userId: int, file_talkId: int, session_id: str, prompt: str):
            chat_sessions[session_id] = {"status": "running"}
            reasoning = 0
            full_response = ""

            try:
                response = await client.chat.completions.create(
                    model="deepseek-reasoner",
                    messages=prompt,
                    stream=True
                )
                async for chunk in response:
                    if chat_sessions[session_id]["status"] == "stopped":
                        await client.aclose()
                        break
                    while chat_sessions[session_id]["status"] == "paused":
                        await asyncio.sleep(0.1)
                    if chunk.choices[0].delta.reasoning_content:
                        if reasoning == 0:
                            yield "ğŸ¤”æ­£åœ¨æ€è€ƒ... ...  \n"
                            full_response += "ğŸ¤”æ­£åœ¨æ€è€ƒ... ...  \n"
                            reasoning = 1
                        content = chunk.choices[0].delta.reasoning_content
                        full_response += content
                        yield content
                    if chunk.choices[0].delta.content:
                        if reasoning in [0, 1]:
                            yield "  \n  \nğŸ˜¶ \nğŸ’¬å¼€å§‹å›ç­”:  \n"
                            full_response += "  \n  \nğŸ˜¶ \nğŸ’¬å¼€å§‹å›ç­”:  \n"
                            reasoning = 2
                        content = chunk.choices[0].delta.content
                        full_response += content
                        yield content
            except Exception as e:
                yield f"data: {str(e)}\n\n"
            finally:
                await client.aclose()

            index = prompt[-1]['content'].find("\næœ¬è½®çš„ç”¨æˆ·æé—®æ˜¯ï¼š")
            result = prompt[-1]['content'][index + len("\næœ¬è½®çš„ç”¨æˆ·æé—®æ˜¯ï¼š"):].strip() if index != -1 else ""
            insert_talk_vectors_to_db(file_talkId, file_userId, result, full_response, connect_db, db_params)

        return StreamingResponse(generate_response(file_userId, file_talkId, session_id, messages), media_type='text/plain; charset=utf-8')

    except Exception as e:
        return JSONResponse(status_code=500, content={"msg": str(e), "code": 500, "data": "æ“ä½œå¤±è´¥"})

# æ§åˆ¶æ¥å£
@app.post("/control")
async def control_chat(request_data: ControlRequest):
    session_id = request_data.session_id
    action = request_data.action

    if session_id not in chat_sessions:
        return JSONResponse(content={"error": "Session not found"}, status_code=404)

    if action in ["pause", "resume", "stop"]:
        chat_sessions[session_id]["status"] = "paused" if action == "pause" else "running" if action == "resume" else "stopped"
    else:
        return JSONResponse(content={"error": "Invalid action"}, status_code=400)

    return JSONResponse(content={"message": f"Session {session_id} {action}d successfully."})

# æœ¬åœ°è°ƒè¯•å¯åŠ¨
if __name__ == "__main__":
    import uvicorn
    uvicorn.run("stm_r_talk_asyn_xyjx:app", host="0.0.0.0", port=8058, reload=True)
