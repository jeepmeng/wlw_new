from fastapi import FastAPI, HTTPException, Request
from fastapi.responses import StreamingResponse, JSONResponse
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from openai import OpenAI
import asyncio
import time
import json
import re
from sentence_transformers import SentenceTransformer
from search import query_similar_sentences
from history import his_talk
from pg_interact import insert_vectors_to_db, read_yaml, insert_ques_batch, update_by_id, connect_db
from talk_insert_pg import insert_talk_vectors_to_db
from collections import defaultdict
from fastapi.concurrency import run_in_threadpool
from openai import AsyncOpenAI
import logging
import threading

yaml_file = "config.yaml.bak"  # æŒ‡å®š YAML æ–‡ä»¶è·¯å¾„
db_params = read_yaml(yaml_file)
# é…ç½®æ—¥å¿—æ ¼å¼ï¼ŒåŒ…å«çº¿ç¨‹ ID
logging.basicConfig(
    format="%(asctime)s - %(levelname)s - [Thread: %(threadName)s] - %(message)s",
    level=logging.INFO
)

logger = logging.getLogger(__name__)

app = FastAPI()

# å…è®¸æ‰€æœ‰åŸŸè·¨åŸŸè®¿é—®
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# OpenAI DeepSeek API é…ç½®


ali_client = OpenAI(
    api_key="sk-e8d4973beecd4a43bdce4718b0b2444c",
    base_url="https://dashscope.aliyuncs.com/compatible-mode/v1",
)
model = SentenceTransformer('/data/xht_test_code/FlagEmbedding-master/BAAI/bge-large-zh-v1.5', device="cpu")

# ç”¨äºå­˜å‚¨å½“å‰ä¼šè¯çš„çŠ¶æ€
chat_sessions = {}


class ChatRequest(BaseModel):
    session_id: str
    file_text: str
    file_userId: int
    file_talkId: int


class ControlRequest(BaseModel):
    session_id: str
    action: str  # "pause", "resume", "stop"


# è®°å½•æ˜¯å¦æ˜¯ç¬¬ä¸€æ¬¡è¾“å‡º


@app.post("/roundTalk")
async def multiple_rounds_chat(request_data: ChatRequest):
    client = AsyncOpenAI(api_key="sk-3cd962207189494f872bdec8394a1ffe", base_url="https://api.deepseek.com")
    print(request_data)
    result_data = []
    file_text = request_data.file_text
    file_userId = request_data.file_userId
    file_talkId = request_data.file_talkId
    session_id = request_data.session_id
    print(f'file_text:   {file_text}\n',
          f'file_userId:   {file_userId}',
          f'file_talkId:   {file_talkId}',
          f'session_id:   {session_id}', )

    thread_id = threading.get_ident()
    print(f'å¤„ç†è¯·æ±‚çš„çº¿ç¨‹:{thread_id}')
    thread_name = threading.current_thread().name
    logger.info(f"å¤„ç†è¯·æ±‚çš„çº¿ç¨‹ ID: {thread_id}, çº¿ç¨‹åç§°: {thread_name}")

    try:
        # ç”Ÿæˆæ–‡æœ¬å‘é‡
        vector = model.encode([file_text], normalize_embeddings=True).tolist()[0]
        # print(vector)
        # æ‰§è¡Œæ•°æ®åº“æŸ¥è¯¢
        ques_res = query_similar_sentences(vector, connect_db, db_params)
        # print('ques_res is done')
        res = his_talk(file_userId, file_talkId, connect_db, db_params)

        # è°ƒç”¨é˜¿é‡Œäº‘APIç”Ÿæˆå…³é”®è¯
        prompt = "æ ¹æ®ä¸Šä¼ å†…å®¹è¿›è¡Œå…³é”®è¯æå–ï¼Œåªè¿”å›æå–åçš„å…³é”®è¯ï¼Œç”¨é€—å·åˆ†éš”ï¼Œä¸è¦å…¶ä»–æ–‡å­—"
        completion = ali_client.chat.completions.create(model="qwen-long", messages=[
            {'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'system', 'content': file_text},
            {'role': 'user', 'content': prompt}], stream=True, stream_options={"include_usage": True})
        # å¤„ç†æµå¼å“åº”
        full_content = ""
        for chunk in completion:
            if chunk.choices and chunk.choices[0].delta.content:
                full_content += chunk.choices[0].delta.content
        # å¤„ç†å…³é”®è¯æŸ¥è¯¢
        # print("33333")
        a = [i.strip() for i in full_content.split(",") if i.strip()]
        new_a = []
        for i in a:
            encoded_i = model.encode([i], normalize_embeddings=True).tolist()[0]
            new_a.append(query_similar_sentences(encoded_i, connect_db, db_params))
        # æ„å»ºæç¤ºè¯é€»è¾‘
        new_list = new_a[0] if len(new_a) > 1 else []
        messages = list(res)

        # æ— ç›¸å…³ç»“æœæ—¶çš„å¤„ç†é€»è¾‘
        zhishi_prompt = "".join(new_list)
        wenti_prompt = "".join([i for i in ques_res]) if len(ques_res) > 1 else str(ques_res)
        # print("4-4-4-4-")
        print(f"wenti_prompt----------:{wenti_prompt}")
        new_prompt = (
                f"{zhishi_prompt} \n{wenti_prompt}  \n  \n  ä»¥ä¸Šå†…å®¹æ˜¯ç‰©æ¨¡å‹çš„è§„èŒƒæ–‡æ¡£ï¼Œç”Ÿæˆçš„ç»“æœä¸è¦å¸¦æ³¨é‡Šï¼Œä¸è¦ç¼ºå°‘è§„èŒƒä¸­çš„å„ç§å˜é‡ã€å‚æ•°ã€å®šä¹‰ï¼Œæ¯æ¬¡è¿”å›çš„ç‰©æ¨¡å‹jsonä¸€å®šè¦æ˜¯å…¨éƒ¨çš„ç‰©æ¨¡å‹ç»“æœï¼Œåˆ‡ä¸èƒ½å¤Ÿå¸¦æœ‰æ³¨é‡Šã€‚  \n  \n" + f"\nä»¥ä¸‹æ˜¯æœ¬è½®é—®é¢˜ï¼Œå¦‚æœä¸çŸ¥è¯†åº“ç›¸å…³ï¼Œè¯·ç»“åˆçŸ¥è¯†åº“ä½œç­”ï¼Œå¦‚æœä¸çŸ¥è¯†åº“æ— å…³ï¼Œè¯·ç›´æ¥ä½œç­”ï¼Œå½“å‰ç”¨æˆ·æé—®æ˜¯ï¼š{file_text}")
        # print("4--4--4--4")
        print(f"new_prompt----------:{new_prompt}")
        messages.append({"role": "user", "content": new_prompt})
        # print(messages)
        # response_store = {}  # ç”¨äºå­˜å‚¨å®Œæ•´çš„ clean_full_response






    #     async def generate_response(file_userId: int, file_talkId: int, session_id: str, prompt: str):
    #         # full_response = ""  # ç”¨äºå­˜å‚¨å®Œæ•´å“åº”
    #         chat_sessions[session_id] = {"status": "running"}  # åˆå§‹çŠ¶æ€
    #         reasoning = 0
    #         full_response = []  # ä¿å­˜å®Œæ•´çš„ AI å“åº”å†…å®¹
    #
    #         # **æ¯æ¬¡è°ƒç”¨æ—¶ï¼Œé‡ç½®æ ‡è®°**
    #         first_reasoning_flag = [True]
    #         first_content_flag = [True]
    #         print('å¼€å§‹è¿æ¥deepseek')
    #
    #         try:
    #             response = await client.chat.completions.create(
    #                 model="deepseek-reasoner",
    #                 # messages=[{"role": "user", "content": prompt}],
    #                 messages = prompt,
    #                 stream=True
    #             )
    #             print('ç»“æŸè¿æ¥deepseek')
    #              # âœ… ç›´æ¥éå† `response`ï¼Œå› ä¸ºå®ƒæ˜¯ä¸€ä¸ªæµå¯¹è±¡
    #
    #             async for chunk in response:
    #
    #                 if chat_sessions[session_id]["status"] == "stopped":
    #                     print(f"Session {session_id} è¢«ç»ˆæ­¢")
    #                     client.close()
    #                     break
    #                 while chat_sessions[session_id]["status"] == "paused":
    #                     print(f"Session {session_id} æš‚åœä¸­...")
    #                     await asyncio.sleep(0.1)
    #                 delta = chunk.choices[0].delta
    #                 if delta.reasoning_content is not None:
    #                     if reasoning == 0:
    #                         yield f"ğŸ¤”æ­£åœ¨æ€è€ƒ... ...  \n"
    #                         full_response += "ğŸ¤”æ­£åœ¨æ€è€ƒ... ...  \n"
    #                         reasoning = 1
    #                     # content = chunk.choices[0].delta.reasoning_content
    #                     full_response.append(delta.reasoning_content)
    #                     yield f"{delta.reasoning_content}"
    #                     # await asyncio.sleep(0.1)
    #
    #                 if delta.content is not None:
    #                     if reasoning == 1 or 0:
    #                         yield f"  \n  \nğŸ˜¶ \nğŸ’¬å¼€å§‹å›ç­”:  \n"
    #                         reasoning = 2
    #                         full_response += "  \n  \nğŸ˜¶ \nğŸ’¬å¼€å§‹å›ç­”:  \n"
    #
    #                     # content = chunk.choices[0].delta.content
    #                     # print(content)
    #                     full_response.append(delta.content)
    #                     yield f"{delta.content}"
    #                     # await asyncio.sleep(0.1)
    #
    #         except Exception as e:
    #             print('error')
    #             yield f"data: {str(e)}\n\n"
    #         finally:
    #             await client.close()  # ç¡®ä¿å…³é—­å®¢æˆ·ç«¯è¿æ¥
    #
    #         print('å¼€å§‹å…¥åº“')
    #         # response_store[session_id] = full_response
    #         # print(f'response_store is {response_store[session_id]}')
    #         keyword = "\næœ¬è½®çš„ç”¨æˆ·æé—®æ˜¯ï¼š"  # æŒ‡å®šå­—æ®µ
    #         index = prompt[-1]['content'].find(keyword)
    #
    #         if index != -1:  # æ‰¾åˆ°å…³é”®å­—
    #             result = prompt[-1]['content'][index + len(keyword):].strip()  # æå–åé¢çš„å†…å®¹
    #         else:
    #             result = ""  # å…³é”®å­—ä¸å­˜åœ¨
    #         # full =
    #         print(''.join(full_response))
    #         ## æ’å…¥å¯¹è¯è®°å½•åˆ°æ•°æ®åº“
    #         insert_talk_vectors_to_db(file_talkId,file_userId,result,''.join(full_response),connect_db,db_params)
    #         print('ç»“æŸå…¥åº“')
    #
    #
    #     return StreamingResponse(generate_response(file_userId, file_talkId, session_id,messages), media_type='text/plain; charset=utf-8')
    #
    # except Exception as e:
    #     return JSONResponse(status_code=500, content={"msg": str(e), "code": 500, "data": "æ“ä½œå¤±è´¥"})
    # finally :
    #     pass

    except Exception as e:
        pass
    return messages


@app.post("/control")
async def control_chat(request_data: ControlRequest):
    session_id = request_data.session_id
    action = request_data.action

    if session_id not in chat_sessions:
        return JSONResponse(content={"error": "Session not found"}, status_code=404)

    if action == "pause":
        chat_sessions[session_id]["status"] = "paused"
    elif action == "resume":
        chat_sessions[session_id]["status"] = "running"
    elif action == "stop":
        chat_sessions[session_id]["status"] = "stopped"
    else:
        return JSONResponse(content={"error": "Invalid action"}, status_code=400)

    return JSONResponse(content={"message": f"Session {session_id} {action}d successfully."})


if __name__ == "__main__":
    import uvicorn

    uvicorn.run("stm_r_talk_api_wmx:app", host="0.0.0.0", port=8053, reload=True)
